actions: 4
rewards: [0 0]
next_obs: [5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
actions: 1
rewards: [0 0]
next_obs: [5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
actions: 0
rewards: [0 0]
next_obs: [5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
actions: 5
rewards: [0 0]
next_obs: [5 2 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
actions: 3
rewards: [0 0]
next_obs: [5 2 1 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
actions: 3
rewards: [0 0]
next_obs: [5 2 1 6 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
actions: 3
rewards: [0 0]
next_obs: [5 2 1 6 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0]
actions: 1
rewards: [0 0]
next_obs: [5 2 1 6 4 4 4 2 0 0 0 0 0 0 0 0 0 0 0 0]
actions: 3
rewards: [0 0]
next_obs: [5 2 1 6 4 4 4 2 4 0 0 0 0 0 0 0 0 0 0 0]
actions: 7
rewards: [0 0]
next_obs: [5 2 1 6 4 4 4 2 4 8 0 0 0 0 0 0 0 0 0 0]
actions: 1
rewards: [0 0]
next_obs: [5 2 1 6 4 4 4 2 4 8 2 0 0 0 0 0 0 0 0 0]
actions: 5
rewards: [0 0]
next_obs: [5 2 1 6 4 4 4 2 4 8 2 6 0 0 0 0 0 0 0 0]
actions: 1
rewards: [0 0]
next_obs: [5 2 1 6 4 4 4 2 4 8 2 6 2 0 0 0 0 0 0 0]
actions: 2
rewards: [0 0]
next_obs: [5 2 1 6 4 4 4 2 4 8 2 6 2 3 0 0 0 0 0 0]
actions: 6
rewards: [0 0]
next_obs: [5 2 1 6 4 4 4 2 4 8 2 6 2 3 7 0 0 0 0 0]
actions: 5
rewards: [0 0]
next_obs: [5 2 1 6 4 4 4 2 4 8 2 6 2 3 7 6 0 0 0 0]
actions: 2
rewards: [0 0]
next_obs: [5 2 1 6 4 4 4 2 4 8 2 6 2 3 7 6 3 0 0 0]
actions: 5
rewards: [0 0]
next_obs: [5 2 1 6 4 4 4 2 4 8 2 6 2 3 7 6 3 6 0 0]
actions: 0
rewards: [0 0]
next_obs: [5 2 1 6 4 4 4 2 4 8 2 6 2 3 7 6 3 6 1 0]
actions: 3
rewards: [0 0]
next_obs: [5 2 1 6 4 4 4 2 4 8 2 6 2 3 7 6 3 6 1 4]
actions: 5
rewards: None
next_obs: [5 2 1 6 4 4 4 2 4 8 2 6 2 3 7 6 3 6 1 4]
None
Traceback (most recent call last):
  File "/home/jiajiexu/DT_ECO/RL/test_cgra_dse.py", line 36, in <module>
    agent.train(
  File "/home/jiajiexu/anaconda3/lib/python3.11/site-packages/morl_baselines/single_policy/ser/mosac_dicrete_action.py", line 523, in train
    self.update()
  File "/home/jiajiexu/anaconda3/lib/python3.11/site-packages/morl_baselines/single_policy/ser/mosac_dicrete_action.py", line 410, in update
    _, log_probs, action_probs = self.actor.get_action(mb_obs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiajiexu/anaconda3/lib/python3.11/site-packages/morl_baselines/single_policy/ser/mosac_dicrete_action.py", line 114, in get_action
    action_dist = th.distributions.Categorical(action_probs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiajiexu/anaconda3/lib/python3.11/site-packages/torch/distributions/categorical.py", line 70, in __init__
    super().__init__(batch_shape, validate_args=validate_args)
  File "/home/jiajiexu/anaconda3/lib/python3.11/site-packages/torch/distributions/distribution.py", line 68, in __init__
    raise ValueError(
ValueError: Expected parameter probs (Tensor of shape (128, 9)) of distribution Categorical(probs: torch.Size([128, 9])) to satisfy the constraint Simplex(), but found invalid values:
tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',
       grad_fn=<DivBackward0>)
